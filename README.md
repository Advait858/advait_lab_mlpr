#### 1. What are the common distance metrics used in distance-based classification algorithms? 
In distance-based classification, commonly used distance metrics are Euclidean, Manhattan, and Minkowski. Euclidean measures straight-line distance, Manhattan sums absolute differences across each dimension, and Minkowski generalizes both. Sometimes Cosine similarity is also used, especially in text-based tasks.


#### 2. What are some real-world applications of distance-based classification algorithms?

Distance-based classification is seen in recommendation systems, anomaly detection, medical diagnoses, and image recognition. For instance, KNN can match patient symptoms to known cases for diagnosing illnesses or suggest products by comparing similar user preferences.

#### 3. Explain various distance metrics. 

Euclidean distance measures straight-line separation, Manhattan sums absolute differences, and Minkowski unifies both approaches by introducing a parameter. Meanwhile, Cosine similarity evaluates the angle between vectors, which is particularly useful for textual or high-dimensional data comparisons.

#### 4. What is the role of cross validation in model performance? 

Cross-validation splits data into training and validation folds to gauge how well a model generalizes to unseen data. It mitigates overfitting risks by ensuring that performance estimates are more representative of real-world scenarios.

#### 5. Explain variance and bias in terms of KNN?

In KNN, lower bias means the model closely fits training data, while higher variance makes it sensitive to small data changes. Increasing the number of neighbors usually reduces variance but may raise bias, balancing flexibility and stability.


![image](https://github.com/user-attachments/assets/3b51a85e-2c79-4e16-900a-efe2b0c9bafa)

![Screenshot 2025-02-26 021652](https://github.com/user-attachments/assets/485d29bc-0828-4222-934b-acb4fa7014bf)





